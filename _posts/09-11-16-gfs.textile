---
layout: post
title: GFS
excerpt: Как бы не было смешно обоснования данного вопроса, но все таки Google. Так как Google сегодня самый большой частный владелец информации, сегодня это самый популярный поиск, один из основных игроков интернет сервисов. Объемы хранящейся у них информации растут с колоссальной скоростью. Все это дает Google авторитет в понимании проблем и наилучшего решения.
published: true
categories:
- gfs
- google
---

<object style="margin:0px" width="425" height="355"><param name="movie" value="http://static.slidesharecdn.com/swf/ssplayer2.swf?doc=gfs-091116025918-phpapp01&stripped_title=gfs-2509004" />
<param name="allowFullScreen" value="true"/>
<param name="allowScriptAccess" value="always"/>
<embed src="http://static.slidesharecdn.com/swf/ssplayer2.swf?doc=gfs-091116025918-phpapp01&stripped_title=gfs-2509004" type="application/x-shockwave-flash" allowscriptaccess="always" allowfullscreen="true" width="425" height="355"></embed></object><div style="font-size:11px;font-family:tahoma,arial;height:26px;padding-top:2px;">
View more <a style="text-decoration:underline;" href="http://www.slideshare.net/">presentations</a> from <a style="text-decoration:underline;" href="http://www.slideshare.net/DieuMort">Anton</a>. Download <a style="text-decoration:underline;" href="http://www.slideshare.net/DieuMort/gfs-2509004" title="Gfs">here</a>.</div>

h4. Почему GFS?

Как бы не было смешно обоснования данного вопроса, но все таки Google. Так как Google сегодня самый большой частный владелец информации, сегодня это самый популярный поиск, один из основных игроков интернет сервисов. Объемы хранящейся у них информации растут с колоссальной скоростью. Все это дает Google авторитет в понимании проблем и наилучшего решения.

h4. Проблемы построения DFS

# <strong>Консистентность данных.</strong>
Это одна из основных проблем построения распределенных файловых систем (DFS). При построении DFS для более двух клиентов не противоричивость данных, согласованность друг с дургом достаточно сложно гарантировать, так как в основе  DFS лежит распределенность ключевых узлов системы.
# <strong>Защита от сбоев.</strong>
Обычно DFS строятся из большого количества геторогенного оборудования, по этому не обходимо построение разнообразных мониторингов, систем удаленного доступа, удаленного востоновления, валидации данных, etc. Самым простым способом защиты от потери данных (основная опастность при сбое) являеться репликация данных на разных узлах, тем самым гарантируя востоновления функционирования системы в случае сбоя. При этом опять возникает проблема консистентности данных. 
# <strong>Большие объемы данных.</strong>
Обьем информации растет намного быстрее чем физические возможности накопителей, в связи с этим использование традиционных файловых систем не возможно, а если брать во внимание проблемы озвученные ранее, то обьем хранимых данных увеличиваеться еще в несколько раз за счет решения тех проблем.
# <strong>Высокая скорость доступа.</strong>
Из-за больших обьемов хранимых данных и физических особенностей накопителей не всегда можно обесепечить выскую скорость доступа.

h4. Архитектура GFS

Система представляет из себя 3х звенную архитектуру:  

# Клиент файловой системы взимодействует с мастер нодами и чанк серверами. Имеет основные операции взаимодействия с файловой системой, которые управляются и проходят через мастер ноды;
# Мастер ноды отвечают за управление матаданными всей файловой системы, так же мастер ноды управляют всей глобальной дейтельностью системы, такие как сборка мусора, перемещение чанков между чанк серверами, etc. Мастер постоянно взаимодействует с чанк серверами по средством сервисных сообщений, с помощью которых опрашивает ноды для сбора разнообразных метрик и установки статуса;
# Чанк сервера являются сосбтвенно хранилищами данных (чанков). Чанки имеют фиксированный размер (настраиваемый) и уникальный глобальный 64 битный ключ. Так же данные на чанк серверах реприцируються между нодами, обычно используют до 3х реплик.

Система не имеет совместимость с POSIX API, в связи с этим не имеет Vnode уровня для Unix like систем. Так же разработчики GFS не стали использовать кеширование данных, тем самым избавилисть от проблем с валидностью кеша, все операции кеширования вынесены на более высокие абстракции (BigTable, etc).

h4. Мастер нода

Обычно используется одна мастер нода, что существенно упрощяет архитектуру и позволяет производить сложные перемещения данных, организовывать репликации, использовать глобальные мета данные. В то же время с первого взгляда может показаться, что использование только одного мастер сервера может привести к ботелнекам и может являться узким местом системы, но в Google работают достаточно умные люди, которые прекрасно понимают чего стоит такие решения. Решением данной проблемы являеться грамотное построение взаимодействие клиентов и GFS. Клиенты никогда не читают и не пишут данные через мастер сервер, в место этого они  спрашивают с каким чанк сервером они должны контактировать и дальше они взаимодействуют только с определенным чанк сервером напрямую. 
Мастер нода управляет репликациями чанков, принимает решения о размещении, создает новые чанки, а также координирует различную деятельность внутри системы  для сохранения чанков полностью реплицированными, балансировки нагрузки на чанк-серверы и сборки неиспользуемых ресурсов. 

h4. Метаданные

Мастер хранит три важных вида метаданных: пространства имен файлов и чанков, отображение файла в чанки и положение реплик чанков. Все метаданные хранятся в памяти мастера. Так как метаданные хранятся в памяти, операции мастера выполняются быстро. Состояние дел в системе мастер узнает просто и эффективно. Он выполняется сканирование чанк-серверов в фоновом режиме. Эти периодические сканирования используются для сборки мусора, дополнительных репликаций, в случае обнаружения недоступного чанк-сервера и перемещение чанков, для балансировки нагрузки и свободного места на жестких дисках чанк-серверов.Мастер отслеживает положение чанков. При старте чанк-сервера мастер запоминает его чанки. В процессе работы мастер контролирует все перемещения чанков и состояния чанк-серверов. Таким образом, он обладает всей информацией о положении каждого чанка.Важная часть метаданных — это лог операций. Мастер хранит последовательность операций критических изменений метаданных. По этим отметкам в логе операций, определяется логическое время системы. Именно это логическое время определяет версии файлов и чанков.Так как лог операций важная часть, то он должен надежно храниться, и все изменения в нем должны становиться видимыми для клиентов, только когда изменятся метаданные. Лог операций реплицируется на несколько удаленных машин, и система реагирует на клиентскую операцию, только после сохранения этого лога на диск мастера и диски удаленных машин.Мастер восстанавливает состояние системы, исполняя лог операций. Лог операций сохраняет относительно небольшой размер, сохраняя только последние операции. В процессе работы мастер создает контрольные точки, когда размер лога превосходит некоторой величины, и восстановить систему можно только до ближайшей контрольной точки. Далее по логу можно заново воспроизвести некоторые операции, таким образом, система может откатываться до точки, которая находится между последней контрольной точкой и текущем временем. 

h4. Взаимодействие в нутри системы

# Клиент спрашивает мастера, какой из чанк-серверов владеет чанком, и где находится этот чанк в других репликах. Если необходимо, то мастер отдает чанк кому-то во владение;
# Мастер в ответ выдает первичную реплику, и остальные (вторичные) реплики. Клиент хранит эти данные для дальнейших действий. Теперь, общение с мастером клиенту может понадобиться только, если первичная реплика станет недоступной;
# Далее клиент отсылает данные во все реплики. Он может это делать в произвольном порядке. Каждый чанк-сервер будет их хранить в специальном буфере, пока они не понадобятся или не устареют;
# Когда все реплики примут эти данные, клиент посылает запрос на запись первичной реплике. В этом запросе содержатся идентификация данных, которые были посланы в шаге. Теперь первичная реплика устанавливает порядок, в котором должны выполняться все изменения, которые она получила, возможно от нескольких клиентов параллельно. И затем, выполняет эти изменения локально в этом определенном порядке.
Первичная реплика пересылает запрос на запись всем вторичным репликам;
# Каждая вторичная реплика выполняет эти изменения в порядке, определенном первичной репликой;
# Вторичные реплики рапортуют об успешном выполнении этих операций;
# Первичная реплика шлет ответ клиенту. Любые ошибки, возникшие в какой-либо реплике, также отсылаются клиенту. Если ошибка возникла при записи в первичной реплике, то и запись во вторичные реплики не происходит, иначе запись произошла в первичной реплике, и подмножестве вторичных. В этом случае клиент обрабатывает ошибку и решает, что ему дальше с ней делать.

h4. Сбои системы

Авторы системы считают одной из наиболее сложных проблем частые сбои работы компонентов системы. Количество и качество компонентов делают эти сбои не просто исключением, а скорее нормой. Сбой компонента может быть вызван недоступностью этого компонента или, что хуже, наличием испорченных данных. GFS поддерживает систему в рабочем виде при помощи двух простых стратегий: быстрое восстановление и репликации.Быстрое восстановление — это, фактически, перезагрузка машины. При этом время запуска очень маленькое, что приводит к маленькой заминке, а затем работа продолжается штатно. Про репликации чанков уже говорилось выше. Мастер реплицирует чанк, если одна из реплик стала недоступной, либо повредились данные, содержащие реплику чанка. Поврежденные чанки определяется при помощи вычисления контрольных сумм.Еще один вид репликаций в системе, про который мало было сказано — это репликация мастера. Реплицируется лог операций и контрольные точки (checkpoints). Каждое изменение файлов в системе происходит только после записи лога операций на диски мастером, и диски машин, на которые лог реплицируется. В случае небольших неполадок мастер может перезагрузиться. В случае проблем с жестким диском или другой жизненно важной инфраструктурой мастера, GFS стартует нового мастера, на одной из машин, куда реплицировались данные мастера. Клиенты обращаются к мастеру по DNS, который может быть переназначен новой машине. Новый мастер является тенью старого, а не точной копией. Поэтому у него есть доступ к файлам только для чтения. То есть он не становится полноценным мастером, а лишь поддерживает лог операций и другие структуры мастера. Важной частью системы является возможность поддерживать целостность данных. Обычный GFS кластер состоит из сотен машин, на которых расположены тысячи жестких дисков, и эти диски при работе с завидным постоянством выходят из строя, что приводит к порче данных. Система может восстановить данные с помощью репликаций, но для этого необходимо понять испортились ли данные. Простое сравнение различных реплик на разных чанк-серверах является неэффективным. Более того, может происходить несогласованность данных между различными репликами, ведущая к различию данных. Поэтому каждый чанк-сервер должен самостоятельно определять целостность данных. Каждый чанк разбивается на блоки длиной 64 Кбайт. Каждому такому блоку соответствует 32-битная контрольная сумма. Как и другие метаданные эти суммы хранятся в памяти, регулярно сохраняются в лог, отдельно от данных пользователя. Перед тем как считать данные чанк-сервер проверяет контрольные суммы блоков чанка, которые пересекаются с затребованными данными пользователем или другим чанк-сервером. То есть чанк-сервер не распространяет испорченные данные. В случае несовпадения контрольных сумм, чанк-сервер возвращает ошибку машине, подавшей запрос, и рапортует о ней мастеру. Пользователь может считать данные из другой реплики, а мастер создает еще одну копию из данных другой реплики. После этого мастер дает инструкцию этому чанк-серверу об удалении этой испорченной реплики. При добавлении новых данных, верификация контрольных сумм не происходит, а для блоков записывается новые контрольные суммы. В случае если диск испорчен, то это определится при попытке чтения этих данных. При записи чанк-сервер сравнивает только первый и последний блоки, пересекающиеся с границами, в которые происходит запись, поскольку часть данных на этих блоках не перезаписывается и необходимо проверить их целостность.